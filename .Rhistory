clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
setwd("~/Dropbox/Data project/Kaggle Homesite")
library(readr)
library(xgboost)
library(doParallel)
library(caret)
set.seed(308)
cat("reading the train and test data\n")
train <- read_csv("input/train.csv")
test  <- read_csv("input/test.csv")
# There are some NAs in the integer columns so conversion to zero
train[is.na(train)]   <- -1
test[is.na(test)]   <- -1
# cat("train data column names and details\n")
# names(train)
# str(train)
# summary(train)
# cat("test data column names and details\n")
# names(test)
# str(test)
# summary(test)
# seperating out the elements of the date column for the train set
train$month <- as.integer(format(train$Original_Quote_Date, "%m"))
train$year <- as.integer(format(train$Original_Quote_Date, "%y"))
train$day <- weekdays(as.Date(train$Original_Quote_Date))
# removing the date column
train <- train[,-c(2)]
# seperating out the elements of the date column for the train set
test$month <- as.integer(format(test$Original_Quote_Date, "%m"))
test$year <- as.integer(format(test$Original_Quote_Date, "%y"))
test$day <- weekdays(as.Date(test$Original_Quote_Date))
# removing the date column
test <- test[,-c(2)]
feature.names <- names(train)[c(3:301)]
# cat("Feature Names\n")
# feature.names
cat("assuming text variables are categorical & replacing them with numeric ids\n")
for (f in feature.names) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]], test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))
}
}
# cat("train data column names after slight feature engineering\n")
# names(train)
# cat("test data column names after slight feature engineering\n")
# names(test)
set.seed(9)
gc()
tra<-train[,feature.names]
#tra<-tra[,c(1:50,65:105,110:165,180:230,245:290)]
dim(tra)
dim(test)
nrow(train)
h<-sample(nrow(train),100000)
dval<-xgb.DMatrix(data=data.matrix(tra[h,]),label=train$QuoteConversion_Flag[h])
#dtrain<-xgb.DMatrix(data=data.matrix(tra[-h,]),label=train$QuoteConversion_Flag[-h])
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
watchlist<-list(val=dval,train=dtrain)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
detach("package:doParallel", unload=TRUE)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
install.packages(“Rcpp”)
install.packages('Rcpp')
install.packages('xgboost/R-package/', type='source')
setwd("~/")
install.packages('xgboost/R-package/', type='source')
install.packages('xgboost/R-package/', repos=NULL, type='source')
devtools::install_github("hadley/devtools")
install_github("hadley/devtools")
install.packages("devtools")
devtools::install_local('xgboost/', subdir = 'R-package')
is_using_multithread()
install.packages('mxnet/R-package/', repos=NULL, type='source')
devtools::install_local('mxnet/', subdir = 'R-package')
library(mxnet)
devtools::install_local('mxnet/', subdir = 'R-package')
install.packages('mxnet/R-package/', repos=NULL, type='source')
install.packages('mxnet/R-package/', repos=NULL, type='source')
library(mxnet)
demo(topic = "basic_bench", package = "mxnet")
print(paste0(“Finish prediction… accuracy=”, accuracy(label, pred)))
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
install.packages("forecast")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
library(forecast)
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_bench", package = "mxnet")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_model", package = "mxnet")
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
library(xgboost)
devtools::install_local('xgboost/', subdir = 'R-package')
setwd("~/")
devtools::install_local('xgboost/', subdir = 'R-package')
library(xgboost)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
?n
library(ggmap)
library(ggplot2)
library(dplyr)
library(readr)
?n
install.packages('devtools', repo=\ 'https://cran.rstudio.com')
install.packages('devtools', repo=\'https://cran.rstudio.com')
install.packages('devtools', repo='https://cran.rstudio.com')
library(mxnet)
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv()
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv("DYLD_LIBRARY_PATH")
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
?dyn.load
?n.load
library(mxnet)
require(mxnet)
context("ndarray")
test_that("element-wise calculation for vector", {
x = 1:10
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("element-wise calculation for matrix", {
x = matrix(1:4, 2, 2)
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("ndarray ones, zeros, save and load", {
expect_equal(rep(0, 10), as.array(mx.nd.zeros(10)))
expect_equal(matrix(0, 10, 5), as.array(mx.nd.zeros(c(10, 5))))
expect_equal(rep(1, 10), as.array(mx.nd.ones(10)))
expect_equal(matrix(1, 10, 5), as.array(mx.nd.ones(c(10, 5))))
mat = mx.nd.array(1:20)
mx.nd.save(mat, 'temp.mat')
mat2 = mx.nd.load('temp.mat')
expect_true(is.mx.ndarray(mat2[[1]]))
expect_equal(as.array(mat), as.array(mat2[[1]]))
})
library(testthat)
library(mxnet)
test_check("mxnet")
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
#c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
View(shotSel.def)
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
rm(list = ls())
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
library(ggplot2)
library(knitr)
library(dplyr)
data(ToothGrowth)
kable(head(ToothGrowth))
kable(summary(ToothGrowth))
ggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = factor(supp))) +
geom_boxplot() +
ylab('Tooth length') + xlab('Dose in milligrams/day') +
ggtitle('Impact of Dose and Supplement Type') +
theme_bw(base_size = 15) + scale_fill_discrete(name="Supplement type")+
theme(legend.position = c(0.2, 0.8))
anova <- aov(len ~ supp * dose, data=toothGrowth)
summary(anova)
anova <- aov(len ~ supp * dose, data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)
anova <- aov(len ~ supp * factor(dose), data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)$supp
TukeyHSD(anova)$factor(dose)
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
?group_bt
?group_by
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
library(XML)
library(dplyr)
library(ggplot2)
library(knitr)
library(lubridate)
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
View(eq.df)
swirl()
library(swirl)
swirl()
0.997*0.001
0.003*0.999
5
0.003*0.999
info()
0.015*0.999
0.997*0.001/(0.997*0.001+0.015*0.999)
3.5
expect_dice()
expect_dice(1)
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
0.5*(edh+edl)
integrate(myfunc, 0, 2)
spop
mean(spop)
allsam
apply(allsam, 1, mean)
mean(smeans)
dice_sqr
ex2_fair <- dice_sqr*dice_fair
ex2_fair <- sum(dice_sqr*dice_fair)
ex2_fair-3.5^2
sum(dice_sqr*dice_high)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
sd(apply(matrix(poisson(4,10000),1000),1,mean))
sd(apply(matrix(poisson(4),1000),1,mean))
sd(apply(matrix(rpoisson(4),1000),1,mean))
sd(apply(matrix(rpois(4),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 1000),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 10000),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 10000),10),1,mean))
sd(apply(matrix(rpois(lambda = 4, 100),10),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/2/sqrt(10)
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
library(caret)
?sigma
detach("package:stats", unload=TRUE)
library("stats", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
?stats::sigma
?sigma
?nearZeroVar
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user="root", password='j989mjj8',
host="localhost")
result <- dbGetQuery(ucscDb,"show databases;")
View(result)
dbDisconnect(ucscDb)
ucscDb <- dbConnect(MySQL(),user="root", password='j989mjj8', db='nytaxi',
host="localhost")
dbDisconnect(ucscDb)
hg19 <- dbConnect(MySQL(),user="root", password='j989mjj8', db='nytaxi',
host="localhost")
allTables <- dbListTables(hg19)
length(allTables)
head(allTables)
dbListFields(hg19,allTables[1])
dbGetQuery(hg19, paste0("select count(*) from ", allTables[2]))
q <- dbSendQuery(hg19, paste0("select * from ", allTables[2]))
table <- fetch(q, 10)
table
dbDisconnect(hg19)
View(table)
library(mxnet)
library(twitteR)
library(ROAuth)
library(httr)
install.packages('twitteR')
install.packages("ROAuth")
library(twitteR)
library(ROAuth)
library(httr)
# Set API Keys
api_key <- "4cUUzuj2biWDEPxAJdqznc3Uo"
api_secret <- "fWHr4bkXf0120uJfX2v3j0NqGwsQ9UdA47PCGK89mBSLQDck0g"
access_token <- "119421107-gfS90RWUYNpHwba1suGl99Z8dWOIJq9myHKGRCuR"
access_token_secret <- "jOpvqYDxIYizw2miBV1Vv6ZEVHKCPuuzo5tIw62cDv7N1"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
# Grab latest tweets
tweets_illini <- searchTwitter('illini', n=1500)
# Loop over tweets and extract text
library(plyr)
feed_illini = laply(tweets_illini, function(t) t$getText())
feed_illini <- lapply(tweets_illini, function(t) t$getText())
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
getSentiment <- function (text, key){
text <- URLencode(text);
#save all the spaces, then get rid of the weird characters that break the API, then convert back the URL-encoded spaces.
text <- str_replace_all(text, "%20", " ");
text <- str_replace_all(text, "%\\d\\d", "");
text <- str_replace_all(text, " ", "%20");
if (str_length(text) > 360){
text <- substr(text, 0, 359);
}
##########################################
data <- getURL(paste("http://api.datumbox.com/1.0/TwitterSentimentAnalysis.json?api_key=", key, "&text=",text, sep=""))
js <- fromJSON(data, asText=TRUE);
# get mood probability
sentiment = js$output$result
###################################
return(list(sentiment=sentiment))
}
illini <- clean.text(feed_illini)
feed_illini <- sapply(tweets_illini, function(t) t$getText())
illini <- clean.text(feed_illini)
View(illini)
illini
install.packages("wordcloud")
library("word cloud")
#generate wordcloud
wordcloud(illini,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
library(wordcloud)
#generate wordcloud
wordcloud(illini,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
install.packages("tm")
wordcloud(illini,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
wordcloud(illini,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 50)
illini_corpus = Corpus(VectorSource(illini))
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(illini_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("illini", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(2, "Dark2"))
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(15, "Dark2"))
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud(dm$word, dm$freq,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
wordcloud(dm$word, dm$freq,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 250)
wordcloud(dm$word, dm$freq,min.freq = 2, scale=c(2,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 250)
wordcloud(dm$word, dm$freq,min.freq = 2, scale=c(3,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 250)
wordcloud(dm$word, dm$freq,min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 250)
load("~/OneDrive/Kaggle DSB II/.RData")
rm(ls())
pbinom(14, 25, 0.5, lower.tail = F)
pbinom(15, 25, 0.5, lower.tail = F)
pbinom(12.5, 25, 0.5, lower.tail = F)
pbinom(13, 25, 0.5, lower.tail = F)
pnorm(-0.6)
pbinom(28, 50, 0.5, lower.tail = F)
pbinom(56, 100, 0.5, lower.tail = F)
pnorm(-1.2)
pbinom(5600, 10000, 0.5, lower.tail = F)
pbinom(140, 250, 0.5, lower.tail = F)
pbinom(224, 400, 0.5, lower.tail = F)
pnorm(-2.4)
(51/606 - 0.56)/sqrt(0.56*0.44/606)
(51/606 - 0.56)/sqrt(0.56*0.44/606)
sqrt(0.56*0.44/606)
51/606 - 0.56
51/606
(51/606 - 0.056)/sqrt(0.56*0.44/606)
pnorm(1.39, lower.tail = F)
(51/606 - 0.056)/sqrt(0.056*0.944/606)
pnorm(3.015, lower.tail = F)
pnorm(-2.25)
pnorm(0.34/0.4)
setwd("~/Dropbox/Data project/Car2goRMD")
